# TP_NFC Stability & Performance Improvements

critical_requirement:
  header: "CRITICAL REQUIREMENT"
  description: |
    All improvements must preserve existing functionality completely.
    Read entire codebase before implementing any changes.
  constraints:
    - Current NFC scanning loops and timing is already short. Attempting optimization must be done with caution.
    - All UI modes and transitions must continue working.
    - Thread synchronization patterns must be preserved.
    - No breaking changes to user workflow or interface.

priority_1_code_optimization:
  header: "Priority 1: Code Optimization"
  resource_cleanup:
    header: "Resource Cleanup"
    code_description: "# Add proper context managers"
    code_example: |
      class SafeNFCOperation:
          def __init__(self, nfc_service, operation_name):
              self.nfc_service = nfc_service
              self.operation_name = operation_name
              self.start_time
       = None

          def __enter__(self):
              self.start_time = time.time()
              return self

          def __exit__(self, exc_type, exc_val, exc_tb):
              duration = time.time() - self.start_time
              if duration > 10:  # Log slow operations
                  logger.warning(f"{self.operation_name} took {duration:.2f}s")

priority_2_stability_improvements:
  header: "Priority 2: Stability Improvements"
  error_recovery:
    header: "Error Recovery"
    code_description: "# Add automatic recovery for common failures"
    code_example: |
      class AutoRecovery:

       def __init__(self, max_failures=3, reset_interval=300):
              self.failure_counts = {}
              self.max_failures = max_failures
              self.reset_interval = reset_interval

          def record_failure(self, operation_name):
              now = time.time()
              if operation_name not in self.failure_counts:
                  self.failure_counts[operation_name] = {'count': 0, 'last_reset': now}

              entry =
       self.failure_counts[operation_name]
              if now - entry['last_reset'] > self.reset_interval:
                  entry['count'] = 0
                  entry['last_reset'] = now

              entry['count'] += 1
              return entry['count'] >= self.max_failures
  connection_health_monitoring:
    header: "Connection Health Monitoring"
    code_description: "# Add health checks without disrupting flow"
    code_example: |
      def check_connections_health(self):
          """Non-intrusive health check"""
          health = {

       'nfc_connected': self.nfc_service.is_connected if hasattr(self.nfc_service, 'is_connected') else False,
              'sheets_auth_valid': self._check_sheets_auth(),
              'memory_usage': self._get_memory_usage()
          }

          # Update UI indicator (small dot in corner)
          self._update_health_indicator(health)

          # Schedule next check
          self.after(30000, self.check_connections_health)  # Every 30 seconds

      def _check_sheets_auth(self):
          try:
              # Quick non-invasive check
              return hasattr(self.sheets_service, 'creds') and self.sheets_service.creds.valid

       except:
              return False

priority_3_performance_monitoring:
  header: "Priority 3: Performance Monitoring (Lightweight)"
  basic_metrics_collection:
    header: "Basic Metrics Collection"
    code_example: |
      class PerformanceMonitor:
          def __init__(self):
              self.metrics = {
                  'nfc_read_times': [],
                  'ui_update_times': [],
                  'sheets_call_times': []
              }
              self.max_samples = 100  # Keep only recent samples


          def record_time(self, operation, duration):
              if operation in self.metrics:
                  times = self.metrics[operation]
                  times.append(duration)
                  if len(times) > self.max_samples:
                      times.pop(0)  # Remove oldest

          def get_average_time(self, operation):
              times = self.metrics.get(operation,
       [])
              return sum(times) / len(times) if times else 0
  memory_usage_tracking:
    header: "Memory Usage Tracking"
    code_description: "import psutil\nimport gc"
    code_example: |
      def optimize_memory_usage(self):
          """Periodic memory optimization"""
          # Force garbage collection
          gc.collect()

          # Get current memory usage
          process = psutil.Process()
          memory_mb = process.memory_info().rss / 1024 / 1024

          # Log if memory usage is high
          if memory_mb > 200:  # More than 200MB

       self.logger.warning(f"High memory usage: {memory_mb:.1f}MB")

              # Clear cached data if needed
              if hasattr(self, '_cached_guest_data'):
                  self._cached_guest_data.clear()

          # Schedule next check (every 5 minutes)
          self.after(300000, self.optimize_memory_usage)

priority_4_configuration_optimization:
  header: "Priority 4: Configuration Optimization"
  reduce_ui_update_frequency:
    header: "Reduce UI Update Frequency"
    code_description: "# Batch UI updates to reduce flicker"
    code_example: |
      class UIUpdateBatcher:
          def __init__(self, delay_ms=50):
              self.pending_updates = {}

           self.delay_ms = delay_ms

          def schedule_update(self, update_key, update_func):
              """Batch multiple updates of same type"""
              self.pending_updates[update_key] = update_func
              self.after(self.delay_ms, self._flush_updates)

          def _flush_updates(self):
              for update_func in self.pending_updates.values():
                  try:
                      update_func()

             except Exception as e:
                      self.logger.debug(f"Batched update failed: {e}")
              self.pending_updates.clear()
  optimize_guest_list_updates:
    header: "Optimize Guest List Updates"
    code_example: |
      def optimize_guest_table_refresh(self):
          """Only update changed rows instead of rebuilding entire table"""
          if not hasattr(self, '_last_guest_snapshot'):
              self._last_guest_snapshot = {}

          current_data = {guest.original_id: guest for guest in self.guests_data}

          # Find changes

        for guest_id, guest in current_data.items():
              if guest_id not in self._last_guest_snapshot:
                  self._add_guest_row(guest)  # New guest
              elif self._guest_data_changed(guest, self._last_guest_snapshot[guest_id]):
                  self._update_guest_row(guest)  # Changed guest

          # Remove deleted guests
          for guest_id in self._last_guest_snapshot:
              if guest_id not in current_data:

                self._remove_guest_row(guest_id)

          self._last_guest_snapshot = current_data.copy()

implementation_steps:
  header: "Implementation Steps"
  step_2_thread_management:
    header: "Step 2: Thread Management"
    tasks:
      - Implement resource cleanup contexts.
  step_3_health_monitoring:
    header: "Step 3: Health Monitoring (Week 2)"
    tasks:
      - Add lightweight connection health checks.
      - Implement basic performance metrics.
      - Add memory usage monitoring.
  step_4_ui_optimization:
    header: "Step 4: UI Optimization (Week 3)"
    tasks:
      - Implement UI update batching.
      - Optimize guest list refresh logic.
      - Add configuration for update frequencies.

issues_to_fix:
  header: "Issues to Fix"
  potential_race_conditions:
    header: "Potential Race Conditions"
    list:
      - Station switching during operations: Must complete or cancel active operations.
      - Settings access during scanning: Background scanning continues in checkpoint modes.
      - Rapid button clicks: Operation flags prevent duplicate operations.
      - Widget updates after destruction: Safe update patterns prevent crashes.
  confirmed_race_condition_vulnerabilities:
    header: "Confirmed Race Condition Vulnerabilities"
    list:
      - Reception Checkpoint Processing: Tag processing allows transitions mid-operation, no operation_in_progress blocking.
      - Other Stations Processing: Same as reception - check-in processing doesn't block transitions.
      - Concurrent NFC Operations: Tag Info/Erase can run while background checkpoint scanning active.
      - Rewrite Background vs. Button: Background rewrite scanning + user button operation potential conflict.
      - Tag Info Dual Scanning: Tag Info allows checkpoint scanning = two NFC operations simultaneously.
      - Manual Check-in Concurrency: Quick guest list check-in doesn't set operation_in_progress.
      - State Restoration Corruption: Rapid Settings→Station→Settings could corrupt _came_from_settings flag.

testing_approach:
  header: "Testing Approach"
  stress_testing:
    header: "Stress Testing"
    code_example: |
      def stress_test_scanning():
          """Simulate rapid NFC operations"""
          for i in range(100):
              # Simulate tag detection without actual NFC
              fake_tag = NFCTag(f"TEST{i:04d}")
              self._process_fake_tag(fake_tag)
              time.sleep(0.1)  # 10 tags per second
  memory_leak_detection:
    header: "Memory Leak Detection"
    code_example: |
      def monitor_memory_leaks():
          """Track memory usage over time"""
          import tracemalloc
          tracemalloc.start()

          # Run normal operations...


        snapshot = tracemalloc.take_snapshot()
          top_stats = snapshot.statistics('lineno')[:10]
          for stat in top_stats:
              print(stat)
  edge_cases_to_test:
    header: "Edge Cases to Test"
    list:
      - Reception mode rapid toggle: Registration <-> Checkpoint switching.
      - Settings during background scan: Overlay without stopping background.
      - Network failure during operations: Graceful degradation required.
      - NFC reader disconnect: Connection loss during active operations.
      - Rapid transitions: Settings → Station → Settings → Rewrite chains.
      - Dual NFC reader conflict: Multiple scanning operations to same hardware.

conclusion: "These improvements maintain current functionality while significantly improving stability and performance."